{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Logistic Regression for Gene Expression Data\n",
    "\n",
    "In this lab, we use logistic regression to predict biological characteristics (\"phenotypes\") from gene expression data.  In addition to the concepts in [breast cancer demo](./demo04_breast_cancer.ipynb), you will learn to:\n",
    "* Handle missing data\n",
    "* Perform binary classification, and evaluating performance using various metrics\n",
    "* Perform multi-class logistic classification, and evaluating performance using accuracy and confusion matrix\n",
    "* Use L1-regularization to promote sparse weights for improved estimation (Grad students only)\n",
    "\n",
    "## Background\n",
    "\n",
    "Genes are the basic unit in the DNA and encode blueprints for proteins.  When proteins are synthesized from a gene, the gene is said to \"express\".  Micro-arrays are devices that measure the expression levels of large numbers of genes in parallel.  By finding correlations between expression levels and phenotypes, scientists can identify possible genetic markers for biological characteristics.\n",
    "\n",
    "The data in this lab comes from:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
    "\n",
    "In this data, mice were characterized by three properties:\n",
    "* Whether they had down's syndrome (trisomy) or not\n",
    "* Whether they were stimulated to learn or not\n",
    "* Whether they had a drug memantine or a saline control solution.\n",
    "\n",
    "With these three choices, there are 8 possible classes for each mouse.  For each mouse, the expression levels were measured across 77 genes.  We will see if the characteristics can be predicted from the gene expression levels.  This classification could reveal which genes are potentially involved in Down's syndrome and if drugs and learning have any noticeable effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We begin by loading the standard modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `pd.read_excel` command to read the data from \n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n",
    "\n",
    "into a dataframe `df`.  Use the `index_col` option to specify that column 0 is the index.  Use the `df.head()` to print the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>...</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>Genotype</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Behavior</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MouseID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309_1</th>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_2</th>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_3</th>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_4</th>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_5</th>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_6</th>\n",
       "      <td>0.447506</td>\n",
       "      <td>0.628176</td>\n",
       "      <td>0.367388</td>\n",
       "      <td>2.385939</td>\n",
       "      <td>4.807635</td>\n",
       "      <td>0.218578</td>\n",
       "      <td>0.176233</td>\n",
       "      <td>2.141282</td>\n",
       "      <td>0.195188</td>\n",
       "      <td>1.442398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>0.439833</td>\n",
       "      <td>0.116657</td>\n",
       "      <td>0.140766</td>\n",
       "      <td>0.142180</td>\n",
       "      <td>1.816389</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N   pBRAF_N  \\\n",
       "MouseID                                                                         \n",
       "309_1    0.503644  0.747193  0.430175  2.816329  5.990152  0.218830  0.177565   \n",
       "309_2    0.514617  0.689064  0.411770  2.789514  5.685038  0.211636  0.172817   \n",
       "309_3    0.509183  0.730247  0.418309  2.687201  5.622059  0.209011  0.175722   \n",
       "309_4    0.442107  0.617076  0.358626  2.466947  4.979503  0.222886  0.176463   \n",
       "309_5    0.434940  0.617430  0.358802  2.365785  4.718679  0.213106  0.173627   \n",
       "309_6    0.447506  0.628176  0.367388  2.385939  4.807635  0.218578  0.176233   \n",
       "\n",
       "         pCAMKII_N   pCREB_N    pELK_N   ...     pCFOS_N     SYP_N  H3AcK18_N  \\\n",
       "MouseID                                  ...                                    \n",
       "309_1     2.373744  0.232224  1.750936   ...    0.108336  0.427099   0.114783   \n",
       "309_2     2.292150  0.226972  1.596377   ...    0.104315  0.441581   0.111974   \n",
       "309_3     2.283337  0.230247  1.561316   ...    0.106219  0.435777   0.111883   \n",
       "309_4     2.152301  0.207004  1.595086   ...    0.111262  0.391691   0.130405   \n",
       "309_5     2.134014  0.192158  1.504230   ...    0.110694  0.434154   0.118481   \n",
       "309_6     2.141282  0.195188  1.442398   ...    0.109446  0.439833   0.116657   \n",
       "\n",
       "           EGR1_N  H3MeK4_N    CaNA_N  Genotype  Treatment  Behavior   class  \n",
       "MouseID                                                                       \n",
       "309_1    0.131790  0.128186  1.675652   Control  Memantine       C/S  c-CS-m  \n",
       "309_2    0.135103  0.131119  1.743610   Control  Memantine       C/S  c-CS-m  \n",
       "309_3    0.133362  0.127431  1.926427   Control  Memantine       C/S  c-CS-m  \n",
       "309_4    0.147444  0.146901  1.700563   Control  Memantine       C/S  c-CS-m  \n",
       "309_5    0.140314  0.148380  1.839730   Control  Memantine       C/S  c-CS-m  \n",
       "309_6    0.140766  0.142180  1.816389   Control  Memantine       C/S  c-CS-m  \n",
       "\n",
       "[6 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "filename = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls'\n",
    "df = pd.read_excel(filename,index_col=0)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has missing values.  The site:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "\n",
    "has an excellent summary of methods to deal with missing values.  Following the techniques there, create a new data frame `df1` where the missing values in each column are filled with the mean values from the non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>...</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>Genotype</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Behavior</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MouseID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309_1</th>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.816329</td>\n",
       "      <td>5.990152</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.373744</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.750936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.131790</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.675652</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_2</th>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.411770</td>\n",
       "      <td>2.789514</td>\n",
       "      <td>5.685038</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.292150</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.596377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.743610</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_3</th>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.687201</td>\n",
       "      <td>5.622059</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.283337</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.561316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.926427</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_4</th>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.466947</td>\n",
       "      <td>4.979503</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.152301</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.595086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.700563</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_5</th>\n",
       "      <td>0.434940</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.365785</td>\n",
       "      <td>4.718679</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.134014</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.504230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.148380</td>\n",
       "      <td>1.839730</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309_6</th>\n",
       "      <td>0.447506</td>\n",
       "      <td>0.628176</td>\n",
       "      <td>0.367388</td>\n",
       "      <td>2.385939</td>\n",
       "      <td>4.807635</td>\n",
       "      <td>0.218578</td>\n",
       "      <td>0.176233</td>\n",
       "      <td>2.141282</td>\n",
       "      <td>0.195188</td>\n",
       "      <td>1.442398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>0.439833</td>\n",
       "      <td>0.116657</td>\n",
       "      <td>0.140766</td>\n",
       "      <td>0.142180</td>\n",
       "      <td>1.816389</td>\n",
       "      <td>Control</td>\n",
       "      <td>Memantine</td>\n",
       "      <td>C/S</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DYRK1A_N   ITSN1_N    BDNF_N     NR1_N    NR2A_N    pAKT_N   pBRAF_N  \\\n",
       "MouseID                                                                         \n",
       "309_1    0.503644  0.747193  0.430175  2.816329  5.990152  0.218830  0.177565   \n",
       "309_2    0.514617  0.689064  0.411770  2.789514  5.685038  0.211636  0.172817   \n",
       "309_3    0.509183  0.730247  0.418309  2.687201  5.622059  0.209011  0.175722   \n",
       "309_4    0.442107  0.617076  0.358626  2.466947  4.979503  0.222886  0.176463   \n",
       "309_5    0.434940  0.617430  0.358802  2.365785  4.718679  0.213106  0.173627   \n",
       "309_6    0.447506  0.628176  0.367388  2.385939  4.807635  0.218578  0.176233   \n",
       "\n",
       "         pCAMKII_N   pCREB_N    pELK_N   ...     pCFOS_N     SYP_N  H3AcK18_N  \\\n",
       "MouseID                                  ...                                    \n",
       "309_1     2.373744  0.232224  1.750936   ...    0.108336  0.427099   0.114783   \n",
       "309_2     2.292150  0.226972  1.596377   ...    0.104315  0.441581   0.111974   \n",
       "309_3     2.283337  0.230247  1.561316   ...    0.106219  0.435777   0.111883   \n",
       "309_4     2.152301  0.207004  1.595086   ...    0.111262  0.391691   0.130405   \n",
       "309_5     2.134014  0.192158  1.504230   ...    0.110694  0.434154   0.118481   \n",
       "309_6     2.141282  0.195188  1.442398   ...    0.109446  0.439833   0.116657   \n",
       "\n",
       "           EGR1_N  H3MeK4_N    CaNA_N  Genotype  Treatment  Behavior   class  \n",
       "MouseID                                                                       \n",
       "309_1    0.131790  0.128186  1.675652   Control  Memantine       C/S  c-CS-m  \n",
       "309_2    0.135103  0.131119  1.743610   Control  Memantine       C/S  c-CS-m  \n",
       "309_3    0.133362  0.127431  1.926427   Control  Memantine       C/S  c-CS-m  \n",
       "309_4    0.147444  0.146901  1.700563   Control  Memantine       C/S  c-CS-m  \n",
       "309_5    0.140314  0.148380  1.839730   Control  Memantine       C/S  c-CS-m  \n",
       "309_6    0.140766  0.142180  1.816389   Control  Memantine       C/S  c-CS-m  \n",
       "\n",
       "[6 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "df1 = df.fillna(df.mean())\n",
    "df1.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification for Down's Syndrome\n",
    "\n",
    "We will first predict the binary class label in `df1['Genotype']` which indicates if the mouse has Down's syndrome or not.  Get the string values in `df1['Genotype'].values` and convert this to a numeric vector `y` with 0 or 1.  You may wish to use the `np.unique` command with the `return_inverse=True` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ystr = df1['Genotype'].values #string form of y values\n",
    "u, y = np.unique(ystr,return_inverse=True) #only looking for unique indices (0 or 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predictors, get all but the last four columns of the dataframes.  Standardize the data matrix and call the standardized matrix `Xs`.  The predictors are the expression levels of the 77 genes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31271112  0.5179336   2.2536689  ..., -1.41662394 -1.60789061\n",
      "   1.06590091]\n",
      " [ 0.35679793  0.28650133  1.8802795  ..., -1.32521803 -1.54684392\n",
      "   1.28029118]\n",
      " [ 0.33496588  0.45046461  2.01292763 ..., -1.37325709 -1.62359464\n",
      "   1.85703831]\n",
      " ..., \n",
      " [-0.79192771 -0.88354273 -1.72382963 ...,  1.27078193  3.11724261\n",
      "   0.29352469]\n",
      " [-0.82188815 -0.8130138  -1.52387571 ...,  1.88117889  3.32828966\n",
      "   0.2089962 ]\n",
      " [-0.49491588 -0.62125474 -1.26845332 ...,  1.92748438  3.32672533\n",
      "   0.10478825]]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "X = np.array(df1.loc[:,'DYRK1A_N':'CaNA_N']) #selecting all columns except for last 4\n",
    "Xs = preprocessing.scale(X)\n",
    "print(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LogisticRegression` object `logreg` and `fit` the training data. Use `C = 1e5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(Xs,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the classifer.  That is, use the `logreg.predict` function to predict labels `yhat` and measure the fraction of time that the predictions match the true labels. Also, plot the ROC curve, and measure the AUC. Later, we will properly measure the accuracy and AUC on cross-validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data =  1.0\n",
      "AUC=  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLiISg/4/ZrQ2q1B1rhEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ffb118a7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "yhat = logreg.predict(Xs)\n",
    "acc = np.mean(yhat == y)\n",
    "print(\"Accuracy on training data = \", acc)\n",
    "\n",
    "from sklearn import metrics\n",
    "yprob = logreg.predict_proba(Xs)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y,yprob[:,1])\n",
    "plt.plot(thresholds,tpr, thresholds,fpr)\n",
    "plt.grid()\n",
    "plt.legend(['FPR','TPR'],loc='lower left')\n",
    "\n",
    "auc=metrics.roc_auc_score(y,yprob[:,1])\n",
    "print(\"AUC= \",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a stem plot of the coefficients, `W` in the logistic regression model.  You can get the coefficients from `logreg.coef_`, but you will need to reshape this to a 1D array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1ffb1222898>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ffb1194b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "W=logreg.coef_\n",
    "plt.stem(np.squeeze(W))\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('Feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that `W[i]` is very large for a few components `i`.  These are the genes that are likely to be most involved in Down's Syndrome.  \n",
    "\n",
    "Find the names of the genes for two components `i` where the magnitude of `W[i]` is largest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Two Most Likely Genes are ITSN1_N and BRAF_N\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "w = np.argsort(np.abs(W)).argsort() #needed two argsorts for correctness\n",
    "features = df1.loc[:,'DYRK1A_N':'CaNA_N'].columns.values #extracting names of features\n",
    "\n",
    "maxFeature = features[np.argmax(w)]\n",
    "w = np.delete(w,np.argmax(w)) #removing current max to find next largest element\n",
    "secMaxFeature = features[np.argmax(w)+1] #+1 to maintain original size\n",
    "\n",
    "print('The Two Most Likely Genes are',maxFeature,'and',secMaxFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "The above meaured the accuracy on the training data.  It is more accurate to measure the accuracy on the test data.  Perform 10-fold cross validation and measure the average precision, recall and f1-score, as well as the AUC.  Note, that in performing the cross-validation, you will want to randomly permute the test and training sets using the `shuffle` option.  In this data set, all the samples from each class are bunched together, so shuffling is essential.  Print the mean precision, recall and f1-score and error rate across all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9577, SE=0.0054\n",
      "Recall =    0.9517, SE=0.0085\n",
      "f1 =        0.9545, SE=0.0060\n",
      "Accuracy =  0.9583, SE=0.0054\n",
      "Error Rate = 0.0417, SE=0.0054\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "nfold = 10\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "acc = []\n",
    "error_rate = []\n",
    "\n",
    "for train, test in kf.split(Xs):            \n",
    "    # Get training and test data\n",
    "    Xtr = Xs[train,:]\n",
    "    ytr = y[train]\n",
    "    Xts = Xs[test,:]\n",
    "    yts = y[test]\n",
    "    \n",
    "    # Fit a model\n",
    "    logreg.fit(Xtr, ytr)\n",
    "    yhat = logreg.predict(Xts)\n",
    "    \n",
    "    # Measure performance\n",
    "    preci,reci,f1i,_= precision_recall_fscore_support(yts,yhat,average='binary') \n",
    "    prec.append(preci)\n",
    "    rec.append(reci)\n",
    "    f1.append(f1i)\n",
    "    acci = np.mean(yhat == yts)\n",
    "    acc.append(acci)\n",
    "    error_rate.append(np.mean(yts != yhat))\n",
    "    \n",
    "# Take average values of the metrics\n",
    "precm = np.mean(prec)\n",
    "recm = np.mean(rec)\n",
    "f1m = np.mean(f1)\n",
    "accm= np.mean(acc)\n",
    "error_mean = np.mean(error_rate)\n",
    "\n",
    "# Compute the standard errors\n",
    "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
    "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
    "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
    "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
    "err_se = np.std(error_rate)/np.sqrt(nfold-1)\n",
    "\n",
    "print('Precision = {0:.4f}, SE={1:.4f}'.format(precm,prec_se))\n",
    "print('Recall =    {0:.4f}, SE={1:.4f}'.format(recm, rec_se))\n",
    "print('f1 =        {0:.4f}, SE={1:.4f}'.format(f1m, f1_se))\n",
    "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))\n",
    "print('Error Rate = {0:.4f}, SE={1:.4f}'.format(error_mean, err_se))\n",
    "#error rate is 1-Accuracy with same standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "Now use the response variable in `df1['class']`.  This has 8 possible classes.  Use the `np.unique` funtion as before to convert this to a vector `y` with values 0 to 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "ystr = df1['class'].values\n",
    "u, y = np.unique(ystr, return_inverse = True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a multi-class logistic model by creating a `LogisticRegression` object, `logreg` and then calling the `logreg.fit` method. In general, you could either use the 'one over rest (ovr)' option or the 'multinomial' option. In this exercise use the default 'ovr' and `C=1`. As an optional exercise, you could also compare the results obtained with these two options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(Xs,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data =  0.999074074074\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "yhat = logreg.predict(Xs)\n",
    "acc = np.mean(yhat == y)\n",
    "print(\"Accuracy on training data = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform 10-fold cross validation, and measure the confusion matrix `C` on the test data in each fold. You can use the `confustion_matrix` method in the `sklearn` package.  Add the confusion matrix counts across all folds and then normalize the rows of the confusion matrix so that they sum to one.  Thus, each element `C[i,j]` will represent the fraction of samples where `yhat==j` given `ytrue==i`.  Print the confusion matrix.  You can use the command\n",
    "\n",
    "    print(np.array_str(C, precision=4, suppress_small=True))\n",
    "    \n",
    "to create a nicely formatted print.  Also print the overall mean and SE of the test accuracy across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "The minimum test accuracy rate = 0.988 , Standard Error = 0.0031\n",
      "[[ 0.96    0.0222  0.      0.      0.0222  0.      0.      0.    ]\n",
      " [ 0.      0.9926  0.      0.      0.0074  0.      0.      0.    ]\n",
      " [ 0.      0.      0.9933  0.      0.      0.      0.      0.0074]\n",
      " [ 0.0067  0.      0.      0.9852  0.      0.      0.      0.0074]\n",
      " [ 0.0067  0.0148  0.      0.      0.9778  0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      1.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      1.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# TODO\n",
    "n_y = np.max(y)\n",
    "C = np.zeros((n_y+1,n_y+1))\n",
    "\n",
    "# Create the cross-validation object and error rate matrix\n",
    "nfold = 10\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "err_rate = np.zeros(nfold)\n",
    "\n",
    "# Create the logistic regression object\n",
    "logreg = linear_model.LogisticRegression()\n",
    "\n",
    "# Loop over the folds in the cross-validation\n",
    "for ifold, Ind in enumerate(kf.split(Xs)):        \n",
    "            \n",
    "    # Get training and test data\n",
    "    Itr, Its = Ind\n",
    "    Xtr = Xs[Itr,:]\n",
    "    ytr = y[Itr]\n",
    "    Xts = Xs[Its,:]\n",
    "    yts = y[Its]\n",
    "    \n",
    "    # Fit a model on the training data\n",
    "    logreg.fit(Xtr, ytr)\n",
    "    \n",
    "    # Predict the labels on the test set.\n",
    "    yhat = logreg.predict(Xts)\n",
    "        \n",
    "    # Measure the accuracy\n",
    "    err_rate[ifold] = np.mean(yhat != yts)\n",
    "    C += confusion_matrix(yts,yhat)\n",
    "    print(\"Fold %d\" % ifold)\n",
    "    \n",
    "err_mean = np.mean(err_rate)\n",
    "acc_mean = 1-err_mean\n",
    "sumC = np.sum(C,1)\n",
    "C /= sumC[np.newaxis,:]\n",
    "\n",
    "err_se = np.std(err_rate)/np.sqrt(nfold-1)\n",
    "acc_se = err_se\n",
    "\n",
    "print(\"The minimum test accuracy rate =\",np.round(acc_mean,3),\", Standard Error =\", np.round(acc_se,5))\n",
    "print(np.array_str(C,precision = 4,suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the logistic regression on the entire training data and get the weight coefficients.  This should be a 8 x 77 matrix.  Create a stem plot of the first row of this matrix to see the coefficients on each of the genes for the first class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1ffb1d80550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ffb1d4e320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(Xs,y)\n",
    "W = logreg.coef_\n",
    "#print(W)\n",
    "plt.stem(W[0,:])\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## L1-Regularization\n",
    "\n",
    "Graduate students only complete this section.\n",
    "\n",
    "In most genetic problems, only a limited number of the tested genes are likely influence any particular attribute.  Hence, we would expect that the weight coefficients in the logistic regression model should be sparse.  That is, they should be zero on any gene that plays no role in the particular attribute of interest.  Genetic analysis commonly imposes sparsity by adding an l1-penalty term.  Read the `sklearn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the `LogisticRegression` class to see how to set the l1-penalty and the inverse regularization strength, `C`.\n",
    "\n",
    "Using the model selection strategies from the [prostate cancer analysis demo](../unit03_model_sel/demo03_2_prostate.ipynb), use K-fold cross validation to select an appropriate inverse regularization strength.  \n",
    "* Use 10-fold cross validation \n",
    "* You should select around 20 values of `C`.  It is up to you to find a good range.\n",
    "* For each C and each fold, you should compute the classification error rate \n",
    "* For each C and each fold, you should also determine the nubmer of non-zero coefficients for the first class. For this purpse, you can assume coefficient with magnitude <0.01 as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the mean and standard error on the error rate for each `C` and plot the results (Use `errorbar()` method).  Also determine and print the minimum test error rate and corresponding C value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the minimum error rate is significantly below the classifier that did not use the l1-penalty.  Use the one-standard error rule to determine the optimal `C` and the corresponding test error rate. Note that because `C` is inversely proportional to the regularization strength, you want to select a `C` as *small* as possible while meeting the error target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# C_opt = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does the test error rate compare with the classifier that did not use the l1-penalty? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type Answer Here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the nubmer of non-zero coefficients for the first class for different C values. Also determine and print the number of non-zero coefficients corresponding to C_opt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimal `C`, fit the model on the entire training data with l1 regularization. Find the resulting weight matrix, `W_l1`.  Plot the first row of this weight matrix and compare it to the first row of the weight matrix without the regularization.  You should see that, with l1-regularization, the weight matrix is much more sparse and hence the roles of particular genes are more clearly visible. Please also compare the accuracy for the training data using optimal `C` with the previous results not using LASSO regularization. Do you expect the accuracy to improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
